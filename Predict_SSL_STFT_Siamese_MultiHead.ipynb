{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> - last read : 2019. 06. 3 </div>\n",
    "\n",
    "\n",
    "## Sound Source Location (방위각 추정) and VAD as Multi-task learning\n",
    "- Based on STFT magnitude and phase.  \n",
    "- 2 Models: Convolutional GRU & 2D-CNN then 2D-CNN\n",
    "\n",
    "### Model 시험 함수의 Inputs 형태 : \n",
    "- `predict_utterances(model_path, X, test_idx)` \n",
    "    - model_path : 테스트할 모델의 path\n",
    "    - X : ndarray. 오디오 샘플(파일)들. 가령 50개의 오디오 파일이 있으면 X 의 길이는 50\n",
    "      - 각 오디오 샘플은 1.16 초 길이의 ndarray 형태의 instance 들로 구성.  \n",
    "      - 각 instance는 (512, 100, 4)의 shape를 갖음.  각 instance는 11.6 msec 간격.  \n",
    "        - 512 : height of STFT.\n",
    "        - 100 : number of frames.  Abut 1.16 sec duration\n",
    "        - 4 : Four channels (left mag. left phase, right mag, right phase)\n",
    "    - X[7] : 8번째 오디오 파일.  \n",
    "    - X[7].shape == (17, 512, 100, 4) 이라면 : 8번째 오디오 파일이 17개의 instance로 구성.\n",
    "- test_idx : numpy vector.  Indices of X to consider. \n",
    "\n",
    "\n",
    "### Model 시험 함수의 출력 : \n",
    "- test_idx 길이 만큼의 sample 들 내의 instance들에 대한 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-468b96f06363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Google_Sync'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dev_Exercise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utils'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_utils'"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, glob  \n",
    "import tensorflow as tf\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# To plot pretty figures\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.style.use('ggplot')\n",
    "# plt.rcParams['axes.labelsize'] = 14\n",
    "# plt.rcParams['xtick.labelsize'] = 12\n",
    "# plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph() \n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def reset_keras_session(seed=42):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   # To rid of warnings \n",
    "\n",
    "if sys.platform == 'win32':   # if windows \n",
    "    home = os.path.join('D:', os.sep, 'hblee')   # d:\\hblee\n",
    "    data_repo = os.path.join('D:', os.sep, 'Data_Repo_Win')   # d:\\Data_Repo_Win\n",
    "elif sys.platform == \"linux\" or sys.platform == \"linux2\" :    # if linux \n",
    "    home = os.path.expanduser(\"~\")   # home = os.getenv(\"HOME\")\n",
    "    data_repo = os.path.join(home, 'Data_Repo')\n",
    "    \n",
    "sys.path.append(os.path.join(home, 'Google_Sync', 'Dev_Exercise', 'utils'))\n",
    "from tf_utils import *\n",
    "    \n",
    "from tensorflow import keras \n",
    "keras.__version__, tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "samples : audio samples(files).  50 of them \n",
    "samples_vad_seg : samples segmented as to voice region (1) and non-voice region (0) \n",
    "\n",
    "samples and samples_vad_seg should be aligned.  \n",
    "'''\n",
    "\n",
    "sample_data_repo = os.path.join('..', 'Data', 'sample_data', 't3_audio')\n",
    "samples = glob.glob(os.path.join(sample_data_repo, '**', '*wav'), recursive=True)\n",
    "samples = sorted(samples)   # sort the samples\n",
    "\n",
    "sample_vad_seg_repo = os.path.join('..', 'Data', 'binary_segment')   # 적절하게 변경 필요 \n",
    "samples_vad_seg = glob.glob(os.path.join(sample_vad_seg_repo, '**', '*[npy|npz]'), recursive=True)\n",
    "samples_vad_seg = sorted(samples_vad_seg)   \n",
    "\n",
    "#  Checking \n",
    "print('samples: ', len(samples), samples[25])\n",
    "print('samples segmented: ', len(samples_vad_seg), samples_vad_seg[25])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data set 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_phase(file_path, sr=44100, n_fft=1024, hop_length=512, db=False, n_mels=50) :\n",
    "    \"\"\"\n",
    "    stft의 magnitude와 phase 리턴\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=sr, mono=False)   # 원래의 sr, stereo\n",
    "    DL = librosa.stft(audio[0], n_fft=n_fft, hop_length=hop_length)\n",
    "    DL_mag, DL_phase = librosa.magphase(DL)\n",
    "    \n",
    "    DR = librosa.stft(audio[1], n_fft=n_fft, hop_length=hop_length)\n",
    "    DR_mag, DR_phase = librosa.magphase(DR)\n",
    "    \n",
    "    if db :\n",
    "        DL_mag = librosa.core.amplitude_to_db(DL_mag)\n",
    "        DR_mag = librosa.core.amplitude_to_db(DR_mag)\n",
    "     \n",
    "    # rescale the right magnitudes w.r.t left channel magnitude \n",
    "    avg = DL_mag.mean()     \n",
    "    stdv = DL_mag.std()\n",
    "    DL_mag = (DL_mag - avg)/stdv\n",
    "    DR_mag = (DR_mag - avg)/stdv\n",
    "    \n",
    "    # return( (DL_mag, np.angle(DL_phase)), (DR_mag, np.angle(DR_phase)) )\n",
    "    return( (DL_mag[1:, :], np.angle(DL_phase)[1:, :]), (DR_mag[1:, :], np.angle(DR_phase)[1:, :]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatio_tensor_instances(array_2d, dest_path, seq_len, hop, label):\n",
    "    \"\"\"\n",
    "    array_2d : spectrogram.\n",
    "    seq_len : number of frames in a instance\n",
    "    label : 0 and 1's. The same length as original numpy vector \n",
    "    \"\"\"\n",
    "    row_size, col_size = array_2d.shape[0], array_2d.shape[1]\n",
    "    ratio = len(label)/col_size  # ratio : how many data points per frame \n",
    "    stack_array = []    # 4D tensor that will hold the instances\n",
    "    label_array = []\n",
    "\n",
    "    j=0\n",
    "    while j <= (col_size - (seq_len+1)): \n",
    "        context_frame = array_2d[:, j:(j+seq_len)]\n",
    "        # seg_label = round( label[int(j*ratio):int((j+seq_len)*ratio)].mean() ) \n",
    "        threshold = 0.5  # if greater than the threshold, then speech \n",
    "        seg_label = 1 if label[int(j*ratio):int((j+seq_len)*ratio)].mean() > threshold else 0\n",
    "\n",
    "#         # store the instances\n",
    "#         dest_path_ext = ''.join([dest_path, '_', str(j)])\n",
    "#         os.makedirs(os.path.dirname(dest_path_ext), exist_ok=True)\n",
    "\n",
    "#         np.savez(dest_path_ext, spectrogram = context_frame,\n",
    "#                  label=seg_label)\n",
    "        \n",
    "        stack_array.append(context_frame[:,:,np.newaxis])   # make context_frame to 3d tensor & append \n",
    "        label_array.append(seg_label)\n",
    "            \n",
    "        j = j+hop\n",
    "        \n",
    "    return np.stack(stack_array, axis=0), label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((15, 512, 100, 1), (15, 512, 100, 1), (15,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples = len(samples) \n",
    "\n",
    "mag_L_instances = []    # elements are ndarrays\n",
    "mag_R_instances = []\n",
    "phase_L_instances = []\n",
    "phase_R_instances = []\n",
    "label_instances = []         # elements are lists\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    voice_noise_label = np.load(samples_vad_seg[i])\n",
    "    if('npy' in samples_vad_seg[i].split('/')[-1]):\n",
    "        label = voice_noise_label[0]        # use the left channel label.  this take care of 0 degree problem\n",
    "    else:                                   # npz file\n",
    "        label = voice_noise_label[\"label\"]    \n",
    "    (mag_L, phase_L), (mag_R, phase_R) = mag_phase(samples[i], db=True)\n",
    "    \n",
    "    # generate instances with 1.16 sec duration (100 frames), at every 0.116 sec apart (10 hops)\n",
    "    voice_dest_path = os.path.join(\"mag\", \"Left\", str(i))\n",
    "    mag_L_instances_sub, _ = generatio_tensor_instances(mag_L, voice_dest_path, 100, 10, label)\n",
    "    \n",
    "    voice_dest_path = os.path.join(\"mag\", \"Right\", str(i))\n",
    "    mag_R_instances_sub, _ = generatio_tensor_instances(mag_R, voice_dest_path, 100, 10, label)\n",
    "    \n",
    "    voice_dest_path = os.path.join(\"phase\", \"Left\", str(i))\n",
    "    phase_L_instances_sub, _ = generatio_tensor_instances(phase_L, voice_dest_path, 100, 10, label)\n",
    "    \n",
    "    voice_dest_path = os.path.join(\"phase\", \"Right\", str(i))\n",
    "    phase_R_instances_sub, label_sub = generatio_tensor_instances(phase_R, voice_dest_path, 100, 10, label)\n",
    "    \n",
    "    mag_L_instances.append(mag_L_instances_sub)\n",
    "    mag_R_instances.append(mag_R_instances_sub)\n",
    "    phase_L_instances.append(phase_L_instances_sub)\n",
    "    phase_R_instances.append(phase_R_instances_sub)\n",
    "    \n",
    "    label_instances.append(np.array(label_sub))\n",
    "    \n",
    "\n",
    "print(len(mag_L_instances), len(phase_R_instances), len(label_instances))\n",
    "\n",
    "mag_L_instances[0].shape, phase_R_instances[0].shape, label_instances[0].shape\n",
    "# the first sample produced 15 instances.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, (15, 512, 100, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_instances = []\n",
    "\n",
    "for i in range(0, no_samples):\n",
    "    concat_tensor = np.concatenate([mag_L_instances[i], phase_L_instances[i], \n",
    "                                    mag_R_instances[i], phase_R_instances[i]], axis = -1)\n",
    "    stacked_instances.append(concat_tensor)\n",
    "    \n",
    "len(stacked_instances), stacked_instances[0].shape    # L, R magnitudes and phases are stacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noise와 voice 방향에 따라 labeling\n",
    "- noise : 0                 \n",
    "- 0도 : 1                \n",
    "- 60도 : 2                \n",
    "- 120도 : 3             \n",
    "- 180도 : 4                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "vad_label_instances = copy.deepcopy(label_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12,25):\n",
    "    for j in range(0, len(label_instances[i])):\n",
    "        if(label_instances[i][j] == 1):\n",
    "            label_instances[i][j] = 2\n",
    "            \n",
    "for i in range(25,38):\n",
    "    for j in range(0, len(label_instances[i])):\n",
    "        if(label_instances[i][j] == 1):\n",
    "            label_instances[i][j] = 3\n",
    "            \n",
    "for i in range(38,50):\n",
    "    for j in range(0, len(label_instances[i])):\n",
    "        if(label_instances[i][j] == 1):\n",
    "            label_instances[i][j] = 4\n",
    "            \n",
    "# label_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 822 instances. And we have labeled them into 5 classes.  Let's see how those labels are distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that `stacked_instances` indices has : 0~11(Class-1), 12~24(Class-2), 25~37(Class-3), 38~49(Class-4) and Class-0 is assigned to the noise  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,) (50,) (50,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(stacked_instances)  # transform the list to ndarray\n",
    "\n",
    "y = np.array(label_instances)\n",
    "\n",
    "y_vad = np.array(vad_label_instances)\n",
    "\n",
    "print(X.shape, y.shape, y_vad.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_utterances(model_path, samples_instances, samples_indices) :\n",
    "    \"\"\"\n",
    "    samples_instances : ndarray holding 'samples' number of sample representation in ndarrays. \n",
    "                        Each sample has the shape: (instances_in_sample, 512, 100, 4)\n",
    "    samples_indices : indices of the samples to consider in 'samples_instances'\n",
    "    \"\"\"\n",
    "    labels_pred = [] \n",
    "    \n",
    "    model = keras.models.load_model(model_path)\n",
    "    \n",
    "    X = samples_instances[[samples_indices]]\n",
    "    \n",
    "    for i, sample in enumerate(X) :   # for the instances in each utterance sample \n",
    "        x_L = sample[:, :, :, :2]\n",
    "        x_R = sample[:, :, :, 2:]\n",
    "        \n",
    "        labels_pred.append( np.argmax(model.predict([x_L, x_R]), axis=1) )\n",
    "        \n",
    "    return np.array(labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = sorted(glob.glob(os.path.join('.', 'models', '*.h5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.arange(0, no_samples)\n",
    "# idx = np.random.permutation(no_samples)\n",
    "# test_idx = idx[-10:]\n",
    "test_idx = idx\n",
    "test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions=predict_utterances(models[0], X, test_idx)  # test against the first model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,) (25,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 4),\n",
       " (3, 4),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 0),\n",
       " (3, 0),\n",
       " (3, 0),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 0)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_index = 33\n",
    "print(y[test_idx][sample_index].shape, predictions[sample_index].shape)\n",
    "list(zip(y[test_idx][sample_index], predictions[sample_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_instances_count(sample_instances_labels) : \n",
    "    \"\"\"\n",
    "    sample_instances_labels : class labels for the instances\n",
    "    returns a list where elements are : (class_label, count)\n",
    "    \"\"\"\n",
    "    # sample_instances_labels : class labels for the instances \n",
    "    import operator\n",
    "    unique, counts = np.unique(sample_instances_labels, return_counts=True)\n",
    "    dict_temp = dict(zip(unique, counts))\n",
    "    return sorted(dict_temp.items(), key=operator.itemgetter(1), reverse=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 19), (0, 4), (4, 2)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_instances_count(predictions[sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of models\n",
    "- For each model, do (instance) predictions for all the sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred = predict_utterances(models[0], X, test_idx)\n",
    "\n",
    "for model in models[1: ] :\n",
    "    predictions=predict_utterances(model, X, test_idx)\n",
    "    for sample_ind in range(0, len(test_idx)):\n",
    "        ensemble_pred[sample_ind] = np.concatenate([ensemble_pred[sample_ind], predictions[sample_ind]], \n",
    "                                                   axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0~11(Class-1), 12~24(Class-2), 25~37(Class-3), 38~49(Class-4) and Class-0 is assigned to the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 2),\n",
       " (13, 2),\n",
       " (14, 2),\n",
       " (15, 2),\n",
       " (16, 2),\n",
       " (17, 2),\n",
       " (18, 2),\n",
       " (19, 2),\n",
       " (20, 2),\n",
       " (21, 2),\n",
       " (22, 2),\n",
       " (23, 2),\n",
       " (24, 2),\n",
       " (25, 3),\n",
       " (26, 3),\n",
       " (27, 3),\n",
       " (28, 3),\n",
       " (29, 3),\n",
       " (30, 3),\n",
       " (31, 3),\n",
       " (32, 3),\n",
       " (33, 3),\n",
       " (34, 3),\n",
       " (35, 3),\n",
       " (36, 3),\n",
       " (37, 3),\n",
       " (38, 4),\n",
       " (39, 4),\n",
       " (40, 4),\n",
       " (41, 4),\n",
       " (42, 4),\n",
       " (43, 4),\n",
       " (44, 4),\n",
       " (45, 4),\n",
       " (46, 4),\n",
       " (47, 4),\n",
       " (48, 4),\n",
       " (49, 4)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_predictions = []\n",
    "\n",
    "for instances_labels in ensemble_pred :\n",
    "    for class_label, class_count in class_instances_count(instances_labels) :\n",
    "        if class_label != 0 :\n",
    "            ensemble_predictions.append(class_label)\n",
    "            break\n",
    "        else :\n",
    "            continue\n",
    "            \n",
    "\n",
    "list(zip(range(0, 50), ensemble_predictions))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf.kr]",
   "language": "python",
   "name": "conda-env-tf.kr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
